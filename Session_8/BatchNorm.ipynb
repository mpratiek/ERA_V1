{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aO-7t1Y7-hV4"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kH16rnZ7wt_"
      },
      "source": [
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ky3f_Odl-7um"
      },
      "source": [
        "## Data Transformations\n",
        "\n",
        "We first start with defining our data transformations. We need to think what our data is and how can we augment it to correct represent images which it might not see otherwise.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtssFUKb-jqx"
      },
      "source": [
        "train_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),  # Convert PIL image to PyTorch tensor\n",
        "    transforms.ColorJitter(brightness=0.10, contrast=0.1, saturation=0.10, hue=0.1),\n",
        "    transforms.RandomRotation((-7.0, 7.0), fill=(1,)),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))# Normalize tensor values to [-1, 1]\n",
        "])\n",
        "\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),  # Convert PIL image to PyTorch tensor\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))# Normalize tensor values to [-1, 1]\n",
        "])\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQciFYo2B1mO"
      },
      "source": [
        "# Dataset and Creating Train/Test Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4A84rlfDA23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6842f511-06cd-48a4-fc16-2e2bfbb11377"
      },
      "source": [
        "train = datastrain = datasets.CIFAR10('./data', train=True, download=True, transform=train_transform)\n",
        "test = datasets.CIFAR10('./data', train=False, download=True, transform=test_transform)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgldp_3-Dn0c"
      },
      "source": [
        "# Dataloader Arguments & Test/Train Dataloaders\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8OLDR79DrHG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f00b7f8-b382-4171-f499-6667259129fe"
      },
      "source": [
        "SEED = 1\n",
        "\n",
        "# CUDA?\n",
        "cuda = torch.cuda.is_available()\n",
        "print(\"CUDA Available?\", cuda)\n",
        "\n",
        "# For reproducibility\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "if cuda:\n",
        "    torch.cuda.manual_seed(SEED)\n",
        "\n",
        "# dataloader arguments - something you'll fetch these from cmdprmt\n",
        "# dataloader_args = dict(shuffle=True, batch_size=128, num_workers=4, pin_memory=True) if cuda else dict(shuffle=True, batch_size=64)\n",
        "\n",
        "# # train dataloader\n",
        "# train_loader = torch.utils.data.DataLoader(train, **dataloader_args)\n",
        "\n",
        "# # test dataloader\n",
        "# test_loader = torch.utils.data.DataLoader(test, **dataloader_args)\n",
        "\n",
        "\n",
        "train_dataloader = torch.utils.data.DataLoader(train, batch_size=64, shuffle=True, num_workers=0, pin_memory=True)\n",
        "test_dataloader = torch.utils.data.DataLoader(test, batch_size=64, shuffle=False, num_workers=0, pin_memory=True)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA Available? True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubQL3H6RJL3h"
      },
      "source": [
        "# The model\n",
        "Let's start with the model we first saw"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FXQlB9kH1ov"
      },
      "source": [
        "import torch.nn.functional as F\n",
        "dropout_value = 0.1\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        # Input Block\n",
        "        self.convblock1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3, out_channels=16, kernel_size=(3, 3), padding=2, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.Dropout(dropout_value)\n",
        "        ) # output_size = 34\n",
        "\n",
        "        # CONVOLUTION BLOCK 1\n",
        "        self.convblock2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=(3, 3), padding=0, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.Dropout(dropout_value)\n",
        "        ) # output_size = 32\n",
        "\n",
        "        # TRANSITION BLOCK 1\n",
        "        self.convblock3 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=32, out_channels=10, kernel_size=(1, 1), padding=0, bias=False),\n",
        "        )\n",
        "        self.pool1 = nn.MaxPool2d(2, 2) # output_size = 16\n",
        "\n",
        "        # CONVOLUTION BLOCK 2\n",
        "        self.convblock4 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=10, out_channels=16, kernel_size=(3, 3), padding=1, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.Dropout(dropout_value)\n",
        "        ) # output_size = 16\n",
        "        self.convblock5 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=(3, 3), padding=0, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.Dropout(dropout_value)\n",
        "        ) # output_size = 14\n",
        "        self.convblock6 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=32, out_channels=10, kernel_size=(1, 1), padding=0, bias=False),\n",
        "        )\n",
        "\n",
        "        self.pool2  = nn.MaxPool2d(2, 2) # output_size = 7\n",
        "\n",
        "        self.convblock7 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=10, out_channels=16, kernel_size=(3, 3), padding=1, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.Dropout(dropout_value)\n",
        "        ) # output_size = 7\n",
        "\n",
        "        self.convblock8 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=(3, 3), padding=0, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.Dropout(dropout_value)\n",
        "        )\n",
        "        # output_size = 5\n",
        "\n",
        "        self.convblock9 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(3, 3), padding=0, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.Dropout(dropout_value)\n",
        "        )\n",
        "        # output_size = 3\n",
        "\n",
        "        # OUTPUT BLOCK\n",
        "        self.gap = nn.Sequential(\n",
        "            nn.AvgPool2d(kernel_size=3)\n",
        "        ) # output_size = 1\n",
        "\n",
        "        self.convblock10 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=32, out_channels=10, kernel_size=(1, 1), padding=0, bias=False),\n",
        "            # nn.BatchNorm2d(10),\n",
        "            # nn.ReLU(),\n",
        "            # nn.Dropout(dropout_value)\n",
        "        )\n",
        "\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout_value)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.convblock1(x)\n",
        "        x = self.convblock2(x)\n",
        "        x = self.convblock3(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.convblock4(x)\n",
        "        x = self.convblock5(x)\n",
        "        x = self.convblock6(x)\n",
        "        x = self.pool2(x)\n",
        "        x = self.convblock7(x)\n",
        "        x = self.convblock8(x)\n",
        "        x = self.convblock9(x)\n",
        "        x = self.gap(x)\n",
        "        x = self.convblock10(x)\n",
        "\n",
        "        x = x.view(-1, 10)\n",
        "        return F.log_softmax(x, dim=-1)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3-vp8X9LCWo"
      },
      "source": [
        "# Model Params\n",
        "Can't emphasize on how important viewing Model Summary is.\n",
        "Unfortunately, there is no in-built model visualizer, so we have to take external help"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5skB97zIJQQe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ad5d586-71ab-4582-be69-e7470f7bc3d6"
      },
      "source": [
        "!pip install torchsummary\n",
        "from torchsummary import summary\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "print(device)\n",
        "model = Net().to(device)\n",
        "summary(model, input_size=(3, 32,32))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.10/dist-packages (1.5.1)\n",
            "cuda\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 16, 34, 34]             432\n",
            "              ReLU-2           [-1, 16, 34, 34]               0\n",
            "       BatchNorm2d-3           [-1, 16, 34, 34]              32\n",
            "           Dropout-4           [-1, 16, 34, 34]               0\n",
            "            Conv2d-5           [-1, 32, 32, 32]           4,608\n",
            "              ReLU-6           [-1, 32, 32, 32]               0\n",
            "       BatchNorm2d-7           [-1, 32, 32, 32]              64\n",
            "           Dropout-8           [-1, 32, 32, 32]               0\n",
            "            Conv2d-9           [-1, 10, 32, 32]             320\n",
            "        MaxPool2d-10           [-1, 10, 16, 16]               0\n",
            "           Conv2d-11           [-1, 16, 16, 16]           1,440\n",
            "             ReLU-12           [-1, 16, 16, 16]               0\n",
            "      BatchNorm2d-13           [-1, 16, 16, 16]              32\n",
            "          Dropout-14           [-1, 16, 16, 16]               0\n",
            "           Conv2d-15           [-1, 32, 14, 14]           4,608\n",
            "             ReLU-16           [-1, 32, 14, 14]               0\n",
            "      BatchNorm2d-17           [-1, 32, 14, 14]              64\n",
            "          Dropout-18           [-1, 32, 14, 14]               0\n",
            "           Conv2d-19           [-1, 10, 14, 14]             320\n",
            "        MaxPool2d-20             [-1, 10, 7, 7]               0\n",
            "           Conv2d-21             [-1, 16, 7, 7]           1,440\n",
            "             ReLU-22             [-1, 16, 7, 7]               0\n",
            "      BatchNorm2d-23             [-1, 16, 7, 7]              32\n",
            "          Dropout-24             [-1, 16, 7, 7]               0\n",
            "           Conv2d-25             [-1, 32, 5, 5]           4,608\n",
            "             ReLU-26             [-1, 32, 5, 5]               0\n",
            "      BatchNorm2d-27             [-1, 32, 5, 5]              64\n",
            "          Dropout-28             [-1, 32, 5, 5]               0\n",
            "           Conv2d-29             [-1, 32, 3, 3]           9,216\n",
            "             ReLU-30             [-1, 32, 3, 3]               0\n",
            "      BatchNorm2d-31             [-1, 32, 3, 3]              64\n",
            "          Dropout-32             [-1, 32, 3, 3]               0\n",
            "        AvgPool2d-33             [-1, 32, 1, 1]               0\n",
            "           Conv2d-34             [-1, 10, 1, 1]             320\n",
            "================================================================\n",
            "Total params: 27,664\n",
            "Trainable params: 27,664\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 2.05\n",
            "Params size (MB): 0.11\n",
            "Estimated Total Size (MB): 2.17\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1__x_SbrL7z3"
      },
      "source": [
        "# Training and Testing\n",
        "\n",
        "All right, so we have 24M params, and that's too many, we know that. But the purpose of this notebook is to set things right for our future experiments.\n",
        "\n",
        "Looking at logs can be boring, so we'll introduce **tqdm** progressbar to get cooler logs.\n",
        "\n",
        "Let's write train and test functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbkF2nN_LYIb"
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "train_acc = []\n",
        "test_acc = []\n",
        "\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "  model.train()\n",
        "  pbar = tqdm(train_loader)\n",
        "  correct = 0\n",
        "  processed = 0\n",
        "  for batch_idx, (data, target) in enumerate(pbar):\n",
        "    # get samples\n",
        "    data, target = data.to(device), target.to(device)\n",
        "\n",
        "    # Init\n",
        "    optimizer.zero_grad()\n",
        "    # In PyTorch, we need to set the gradients to zero before starting to do backpropragation because PyTorch accumulates the gradients on subsequent backward passes.\n",
        "    # Because of this, when you start your training loop, ideally you should zero out the gradients so that you do the parameter update correctly.\n",
        "\n",
        "    # Predict\n",
        "    y_pred = model(data)\n",
        "\n",
        "    # Calculate loss\n",
        "    loss = F.nll_loss(y_pred, target)\n",
        "    train_losses.append(loss)\n",
        "\n",
        "    # Backpropagation\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Update pbar-tqdm\n",
        "\n",
        "    pred = y_pred.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "    correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "    processed += len(data)\n",
        "\n",
        "    pbar.set_description(desc= f'Loss={loss.item()} Batch_id={batch_idx} Accuracy={100*correct/processed:0.2f}')\n",
        "    train_acc.append(100*correct/processed)\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    test_losses.append(test_loss)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "\n",
        "    test_acc.append(100. * correct / len(test_loader.dataset))"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aE5Le6FYHhc8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd4118de-548c-4656-9286-8eae63ec2406"
      },
      "source": [
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "model =  Net().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "scheduler = StepLR(optimizer, step_size=6, gamma=0.1)\n",
        "\n",
        "\n",
        "EPOCHS = 20\n",
        "for epoch in range(EPOCHS):\n",
        "    print(\"EPOCH:\", epoch)\n",
        "    train(model, device, train_dataloader, optimizer, epoch)\n",
        "    scheduler.step()\n",
        "    test(model, device, test_dataloader)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=1.2470074892044067 Batch_id=781 Accuracy=44.20: 100%|██████████| 782/782 [01:56<00:00,  6.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.2765, Accuracy: 5320/10000 (53.20%)\n",
            "\n",
            "EPOCH: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=1.3504290580749512 Batch_id=781 Accuracy=56.02: 100%|██████████| 782/782 [01:53<00:00,  6.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.1004, Accuracy: 6086/10000 (60.86%)\n",
            "\n",
            "EPOCH: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=1.07297945022583 Batch_id=781 Accuracy=60.72: 100%|██████████| 782/782 [01:52<00:00,  6.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.0130, Accuracy: 6474/10000 (64.74%)\n",
            "\n",
            "EPOCH: 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=1.2604032754898071 Batch_id=781 Accuracy=63.10: 100%|██████████| 782/782 [01:50<00:00,  7.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.9163, Accuracy: 6726/10000 (67.26%)\n",
            "\n",
            "EPOCH: 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=1.031891107559204 Batch_id=781 Accuracy=65.72: 100%|██████████| 782/782 [01:51<00:00,  6.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.9024, Accuracy: 6835/10000 (68.35%)\n",
            "\n",
            "EPOCH: 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=1.370111346244812 Batch_id=781 Accuracy=67.09: 100%|██████████| 782/782 [01:51<00:00,  7.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.8544, Accuracy: 7035/10000 (70.35%)\n",
            "\n",
            "EPOCH: 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.664262056350708 Batch_id=781 Accuracy=70.24: 100%|██████████| 782/782 [01:49<00:00,  7.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.7763, Accuracy: 7269/10000 (72.69%)\n",
            "\n",
            "EPOCH: 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.5552685856819153 Batch_id=781 Accuracy=70.78: 100%|██████████| 782/782 [01:49<00:00,  7.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.7768, Accuracy: 7278/10000 (72.78%)\n",
            "\n",
            "EPOCH: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=1.1880241632461548 Batch_id=781 Accuracy=70.92: 100%|██████████| 782/782 [01:49<00:00,  7.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.7701, Accuracy: 7307/10000 (73.07%)\n",
            "\n",
            "EPOCH: 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.9181932210922241 Batch_id=781 Accuracy=71.12: 100%|██████████| 782/782 [01:51<00:00,  7.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.7534, Accuracy: 7365/10000 (73.65%)\n",
            "\n",
            "EPOCH: 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.8920611143112183 Batch_id=781 Accuracy=71.54: 100%|██████████| 782/782 [01:49<00:00,  7.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.7578, Accuracy: 7374/10000 (73.74%)\n",
            "\n",
            "EPOCH: 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=1.1775106191635132 Batch_id=781 Accuracy=71.67: 100%|██████████| 782/782 [01:51<00:00,  7.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.7513, Accuracy: 7363/10000 (73.63%)\n",
            "\n",
            "EPOCH: 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.6262948513031006 Batch_id=781 Accuracy=72.01: 100%|██████████| 782/782 [01:50<00:00,  7.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.7512, Accuracy: 7392/10000 (73.92%)\n",
            "\n",
            "EPOCH: 13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.8432958126068115 Batch_id=781 Accuracy=71.90: 100%|██████████| 782/782 [01:50<00:00,  7.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.7506, Accuracy: 7379/10000 (73.79%)\n",
            "\n",
            "EPOCH: 14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.6078283786773682 Batch_id=781 Accuracy=72.17: 100%|██████████| 782/782 [01:51<00:00,  7.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.7516, Accuracy: 7397/10000 (73.97%)\n",
            "\n",
            "EPOCH: 15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.7574386596679688 Batch_id=781 Accuracy=72.13: 100%|██████████| 782/782 [01:49<00:00,  7.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.7433, Accuracy: 7384/10000 (73.84%)\n",
            "\n",
            "EPOCH: 16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=1.060014009475708 Batch_id=781 Accuracy=72.27: 100%|██████████| 782/782 [01:49<00:00,  7.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.7457, Accuracy: 7422/10000 (74.22%)\n",
            "\n",
            "EPOCH: 17\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.5755292177200317 Batch_id=781 Accuracy=72.11: 100%|██████████| 782/782 [01:50<00:00,  7.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.7395, Accuracy: 7420/10000 (74.20%)\n",
            "\n",
            "EPOCH: 18\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.8818075060844421 Batch_id=781 Accuracy=72.14: 100%|██████████| 782/782 [01:49<00:00,  7.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.7460, Accuracy: 7404/10000 (74.04%)\n",
            "\n",
            "EPOCH: 19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=1.1504558324813843 Batch_id=781 Accuracy=72.55: 100%|██████████| 782/782 [01:49<00:00,  7.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.7420, Accuracy: 7429/10000 (74.29%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fKeBz_bFDIBd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}